{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# A) Dataset\n",
    "\n",
    "We have chosen the dataset of emails and our task will be the to detect those that are spam. By training our chosen machine learning algorithm with a collection of labelled training data (messages labelled as spam or not), our machine learning pipeline should then be able to effectively classify whether an email in the test set is a spam email or not - and thus is a binary classification task.\n",
    "\n",
    "The format of the dataset is a collection of text files containing the subject and message of each email. These text files are identified as spam by the name of the file - which is 'spmsg' for spam messaged and 'msg' for normal messages. \n",
    "\n",
    "The first step will be to preprocess this labelled data. To do this, we will create a function that will first read the files from the chosen directory containing the dataset, then the data will be mapped so that any text files named 'spmsg' will be given a nul value, and otherwise normal messages will be given a value of '1'. This will make it clear for the machine learning algorithm to comprehend the class of each message when building the pipeline. \n",
    "\n",
    "Using this, a dataframe is created with the contents of the text file as the first column and the label given to each text file as the second column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2602 files read from directory /data/tempstore/spam/bare/part[1-9]\n",
      "+--------------------+-----+\n",
      "|                text|label|\n",
      "+--------------------+-----+\n",
      "|Subject: becoming...|  0.0|\n",
      "|Subject: zero dow...|  1.0|\n",
      "|Subject: how does...|  1.0|\n",
      "|Subject: philosop...|  0.0|\n",
      "|Subject: job - un...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "dataPath = '/data/tempstore/spam/bare/part[1-9]' # directory of data\n",
    "\n",
    "def create_DF( directory ): # function that creates dataframe of the text files (emails) in the directory\n",
    "   ft_RDD = sc.wholeTextFiles(directory) # Read text files from the directory\n",
    "   print('There are {} files read from directory {}'.format(ft_RDD.count(),directory))# Count the number of files in the directory and display them\n",
    "   spam_text_RDD = ft_RDD.map(lambda ft: (ft[1], 0.0 if re.search('spmsg',ft[0]) is None else 1.0)) # Label spam data as 0 and normal emails 1\n",
    "   DF = spark.createDataFrame(spam_text_RDD, schema=['text','label']) # create a DataFrame\n",
    "   return DF\n",
    "\n",
    "# display our dataframe\n",
    "data = create_DF(dataPath)\n",
    "data.show(5) # Display the top 5 rows\n",
    "\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "-----\n",
    "\n",
    "As you can see, the first column of the dataframe that we have created is the subject and content of each email along with the class / label - where 0 is for a spam email and 1 is for a normal email.\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# B) Machine Learning Pipeline\n",
    "\n",
    "In this section, we will build, train and test our machine learning pipeline. The machine learning algorithm chosen to conduct this classification task is Naive Bayes.\n",
    "\n",
    "First, we'll import the necessary libraries to configure the machine learning pipeline. We then prepare the data by using a 90% training split and a 10% testing split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, IDF\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "# Prepare training and test data:\n",
    "train, test = data.randomSplit([0.9, 0.1], seed=1234) # Randomly split data - controls sequence generation of pseudo-random numbers  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "To configure the pipeline, we first use the tokenisor to transform the data in the first column of the dataframe from a collection of text into the seperate words of the email. We then implemented a hashing function to transform those words from a string of characters into integers and outputted this into a column named 'features'. This allows for easier and faster indexing and retrieval of the data. Both these steps combined with our Naive Bayes as our estimator make up the three steps of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and nb:\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\") # Feature transformer - splits the texts into the words of the text\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\") # Feature extractor - hashes the strings into integer buckets\n",
    "nb = NaiveBayes() # Naive Bayes ML classifier - Our estimator\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, nb]) # 3 stages to the pipeline, feature transforming, extracting and ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# C) Evaluating Performance\n",
    "\n",
    "We then chose the initial values of the parameters for the pipeline as 50 hashing buckets and 0 as the Naive Bayes smoothing parameter. Using this, we then train the classifier on the dataset using the chosen parameters. From this training, the classifier is fit to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineModel_490caf52a5c7c12640b2\n",
      "training:  0.491709759657338\n",
      "PipelineModel_490caf52a5c7c12640b2\n",
      "testing:  0.3628724216959512\n",
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(50,[0,1,2,3,5,6,...|  0.0|       1.0|\n",
      "|(50,[0,1,2,3,4,5,...|  1.0|       1.0|\n",
      "|(50,[0,1,2,3,4,5,...|  0.0|       0.0|\n",
      "|(50,[0,1,4,5,6,9,...|  0.0|       0.0|\n",
      "|(50,[1,2,3,5,6,7,...|  0.0|       0.0|\n",
      "|(50,[0,1,2,3,4,5,...|  1.0|       0.0|\n",
      "|(50,[0,1,2,3,4,5,...|  1.0|       1.0|\n",
      "|(50,[0,1,2,3,4,5,...|  1.0|       1.0|\n",
      "|(50,[0,1,2,3,4,5,...|  0.0|       0.0|\n",
      "|(50,[0,1,2,3,4,5,...|  0.0|       1.0|\n",
      "|(50,[0,1,2,3,4,5,...|  0.0|       0.0|\n",
      "|(50,[0,1,2,3,4,5,...|  0.0|       0.0|\n",
      "|(50,[1,2,3,5,6,8,...|  0.0|       0.0|\n",
      "|(50,[0,1,2,3,4,5,...|  0.0|       0.0|\n",
      "|(50,[0,1,2,3,4,5,...|  0.0|       0.0|\n",
      "|(50,[0,1,2,3,4,5,...|  0.0|       0.0|\n",
      "|(50,[0,1,2,3,4,5,...|  0.0|       0.0|\n",
      "|(50,[0,1,2,3,4,5,...|  0.0|       0.0|\n",
      "|(50,[0,1,2,3,4,6,...|  0.0|       0.0|\n",
      "|(50,[0,1,2,3,4,5,...|  0.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Done\n",
      "Elapsed time is 0:01:25.419502 \n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "starttime = datetime.now()\n",
    "\n",
    "# Implement the ML Pipeline and use a ParamGridBuilder to construct a grid of parameters to test over:\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "   .addGrid(hashingTF.numFeatures, [50]) \\\n",
    "   .addGrid(nb.smoothing, [0]) \\\n",
    "   .build()\n",
    "\n",
    "# Binary Classification Evaluator\n",
    "bc_eval=BinaryClassificationEvaluator() # As this task is a binary classification\n",
    "\n",
    "# Testing using TrainingValidationSplit:\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=bc_eval,\n",
    "                          # 80% of the data will be used for training, 20% for validation.\n",
    "                          trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit:\n",
    "model = tvs.fit(train)\n",
    "\n",
    "# Make predictions on training documents:\n",
    "prediction = model.transform(train)\n",
    "print(model.bestModel)\n",
    "print(\"training: \", bc_eval.evaluate(prediction))\n",
    "\n",
    "\n",
    "# Make predictions on test documents:\n",
    "prediction = model.transform(test)\n",
    "print(model.bestModel)\n",
    "print(\"testing: \", bc_eval.evaluate(prediction))\n",
    "\n",
    "\n",
    "# Make predictions on the testing parameters:\n",
    "model.transform(test)\\\n",
    "   .select(\"features\", \"label\", \"prediction\")\\\n",
    "   .show()\n",
    "   \n",
    "print(\"Done\")\n",
    "\n",
    "\n",
    "endtime = datetime.now()\n",
    "elapsedtime = endtime - starttime\n",
    "print ('Elapsed time is %s ' %elapsedtime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The accuracy of the pipeline model is above, showing approximately 49.2% accuracy for the training set and 36.3% for the test set. The total time taken to train and test the pipeline is just under a minute and a half.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# D) Grid Search\n",
    "\n",
    "Once we have tested that the pipeline works with the data, we will then use a parameter grid that will evaluate the pipeline at different parameter configurations and select the values that return the best results.\n",
    "\n",
    "To implement a parameter grid, we constructed a grid of parameters to search over. The values for the parameter were different hashing buckets of 50, 100, 150 and 200 and a Naive Bayes smoothing values of 0, 1 and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Implement the ML Pipeline and use a ParamGridBuilder to construct a grid of parameters to search over:\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "   .addGrid(hashingTF.numFeatures, [50, 100, 150, 200]) \\\n",
    "   .addGrid(nb.smoothing, [0, 1, 5]) \\\n",
    "   .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As before, we train the model use the TVS function and in this case the model picks the best set of parameters for our Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineModel_418590c848f789bed4d7\n",
      "training:  0.4980520824997364\n",
      "PipelineModel_418590c848f789bed4d7\n",
      "testing:  0.3718869365928191\n",
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(200,[2,5,17,18,2...|  0.0|       1.0|\n",
      "|(200,[0,1,2,3,8,1...|  1.0|       1.0|\n",
      "|(200,[0,1,2,4,5,6...|  0.0|       0.0|\n",
      "|(200,[1,4,9,12,13...|  0.0|       0.0|\n",
      "|(200,[12,17,22,23...|  0.0|       0.0|\n",
      "|(200,[0,1,4,5,6,8...|  1.0|       1.0|\n",
      "|(200,[1,4,5,8,9,1...|  1.0|       1.0|\n",
      "|(200,[0,1,3,4,5,6...|  1.0|       1.0|\n",
      "|(200,[1,2,3,4,6,1...|  0.0|       0.0|\n",
      "|(200,[0,1,2,4,6,7...|  0.0|       1.0|\n",
      "|(200,[0,1,2,3,4,5...|  0.0|       0.0|\n",
      "|(200,[2,3,4,9,10,...|  0.0|       0.0|\n",
      "|(200,[5,9,16,18,1...|  0.0|       0.0|\n",
      "|(200,[1,2,4,5,7,8...|  0.0|       0.0|\n",
      "|(200,[0,1,2,3,4,5...|  0.0|       0.0|\n",
      "|(200,[0,1,2,3,4,5...|  0.0|       0.0|\n",
      "|(200,[1,5,6,7,8,9...|  0.0|       0.0|\n",
      "|(200,[0,1,2,4,7,8...|  0.0|       0.0|\n",
      "|(200,[10,13,15,17...|  0.0|       0.0|\n",
      "|(200,[2,3,4,5,6,7...|  0.0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Done\n",
      "{Param(parent='HashingTF_4fbdab0eb774b9b7b33c', name='numFeatures', doc='number of features.'): 200, Param(parent='NaiveBayes_427893d244946943901b', name='smoothing', doc='The smoothing parameter, should be >= 0, default is 1.0'): 5}\n",
      "--------------\n",
      "Final Elapsed time for grid search 0:06:44.764576 \n"
     ]
    }
   ],
   "source": [
    "starttime=datetime.now()\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using the nb evaluator:\n",
    "tvs = TrainValidationSplit(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=bc_eval,\n",
    "                          # 80% of the data will be used for training, 20% for validation.\n",
    "                          trainRatio=0.8)\n",
    "\n",
    "# Run TrainValidationSplit, and choose the best set of parameters:\n",
    "model = tvs.fit(train)\n",
    "\n",
    "# Make predictions on training documents:\n",
    "prediction = model.transform(train)\n",
    "print(model.bestModel)\n",
    "print(\"training: \", bc_eval.evaluate(prediction))\n",
    "\n",
    "\n",
    "# Make predictions on test documents:\n",
    "prediction = model.transform(test)\n",
    "print(model.bestModel)\n",
    "print(\"testing: \", bc_eval.evaluate(prediction))\n",
    "\n",
    "\n",
    "# Make predictions on the best combination of parameters:\n",
    "model.transform(test)\\\n",
    "   .select(\"features\", \"label\", \"prediction\")\\\n",
    "   .show()\n",
    "   \n",
    "print(\"Done\")\n",
    "\n",
    "# find optimal parameters from grid search\n",
    "evalmetric=model.validationMetrics\n",
    "maxparams=np.argmax(evalmetric) #return parameters that give the highest accuracy\n",
    "print(paramGrid[maxparams])\n",
    "\n",
    "print('--------------')\n",
    "endtime = datetime.now()\n",
    "elapsedtime = endtime - starttime\n",
    "print ('Final Elapsed time for grid search %s ' %elapsedtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "-----\n",
    "There is a marginal improvement in accuracy as a result of the grid search, with the training set accuracy 49.8% and test set accuracy 37.2%. The optimal parameters from the grid search were: 200 hashing buckets and 5 as the smoothing factor. This number of buckets must have been large enough to mitigate some of the collisions that must have occurred with 50 hashing buckets part C. Meanwhile, the laplace factor of greater than 0 removes the problem / bias if the certain words are not present in the training set, which would skew the Naive Bayes MAP calculation. The optimal parameter being 5 must mean that this was otherwise a slight issue in part C where the laplace smoothing parameter was set as 0.\n",
    "\n",
    "Even though the accuracy is quite low, Naive Bayes offered a relatively quick evaluation time just less than  7 minutes to complete the grid search. However, Naive Bayes relies on the independance assumption between instances to hold and better accuracy could have been attained by using a different classification algorithm, such as logistic regression.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work was done in pairs:\n",
    "\n",
    "Islam Ibrahim & Saeed Ahmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
