{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Coursework Part 1: Detecting Spam with Spark\n",
    "\n",
    "IN432 Big Data coursework 2017, part 1. Classifying messages to detect spam. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task a) & b) Read some files and prepare a (f,w) RDD \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hdfs://saltdean.nsqdc.city.ac.uk/data/spam/bare/part1/3-1msg1.txt', 'subject'), ('hdfs://saltdean.nsqdc.city.ac.uk/data/spam/bare/part1/3-1msg1.txt', 're'), ('hdfs://saltdean.nsqdc.city.ac.uk/data/spam/bare/part1/3-1msg1.txt', '2')]\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "# USE DEPENDING ON DATASTORE\n",
    "#prefix = '/data/tempstore/'\n",
    "prefix = 'hdfs://saltdean.nsqdc.city.ac.uk/data/'\n",
    "\n",
    "dirPath = prefix + 'spam/bare/part1'\n",
    "\n",
    "\n",
    "def read_fw_RDD( argDir ): # package tasks a/b into a function for later use\n",
    "    fwL_RDD = sc.wholeTextFiles(argDir) # read the files\n",
    "    #print('Read {} files from directory {}'.format(fwL_RDD,argDir)) # status message for testing, can be disabled later on\n",
    "    #print('file word count histogram') # the histogram can be useful for checking later \n",
    "    #print(fwL_RDD.map(lambda fwL: (len(fwL[1]))).histogram([0,10,100,1000,10000]))\n",
    "    fw_RDD = fwL_RDD.flatMap(lambda f: [(f[0],w.lower()) for w in re.split('\\W+', f[1])]) # prescreate tuples of (filename,word) by splitting the words in the file and lower-casing them\n",
    "    \n",
    "    return fw_RDD # A fw_RDD should be returned\n",
    "\n",
    "fw_RDD = read_fw_RDD(dirPath) # for testing\n",
    "print(fw_RDD.take(3)) # for testing\n",
    "\n",
    "# result returns [(filename, word)....]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task c) Normalised word count lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hdfs://saltdean.nsqdc.city.ac.uk/data/spam/bare/part1/3-550msg1.txt', [('query', 0.045454545454545456), ('annotext', 0.045454545454545456), ('anyone', 0.045454545454545456), ('internet', 0.045454545454545456), ('', 0.045454545454545456), ('michael', 0.045454545454545456), ('subject', 0.045454545454545456), ('thanks', 0.045454545454545456), ('greek', 0.045454545454545456), ('know', 0.045454545454545456), ('classical', 0.045454545454545456), ('dedicated', 0.045454545454545456), ('sikillian', 0.045454545454545456), ('latin', 0.045454545454545456), ('any', 0.045454545454545456), ('does', 0.045454545454545456), ('lists', 0.09090909090909091), ('or', 0.09090909090909091), ('bitnet', 0.045454545454545456), ('to', 0.045454545454545456)])]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "def reGrpLst(fw_c): # reorganise the tuples\n",
    "    fw, c= fw_c\n",
    "    f,w = fw \n",
    "    return (f, [(w,c)]) # Function that will be called to present output in the format (filename, [(word, count)....])\n",
    " \n",
    "def make_f_tfLn_RDD(argDir):  \n",
    "    fw_RDD = read_fw_RDD( argDir ) # call function from task a & b\n",
    "    f_wcL1_RDD=fw_RDD.map(lambda fw: (fw,1))  # add integer 1 to each file,word tuple\n",
    "    f_wcL2_RDD=f_wcL1_RDD.map(lambda fw: fw) \n",
    "    f_wcL3_RDD=f_wcL2_RDD.reduceByKey(add)  #reduce and sum \n",
    "    f_wcL4_RDD=f_wcL3_RDD.map(reGrpLst) #call function that will reorganise tuple\n",
    "    f_wcL5_RDD=f_wcL4_RDD.reduceByKey(add) # reduce and sum \n",
    "    f_wcL6_RDD=f_wcL5_RDD.map(lambda f_wcl: (f_wcl[0], sum([c for (w,c) in f_wcl[1]]))) #create RDD with (filename ([total count]))\n",
    "    f_wcL7_RDD=f_wcL5_RDD.join(f_wcL6_RDD) #(join filename([total count] ) with filename ([word, count]...)\n",
    "    f_wcLn_RDD=f_wcL7_RDD.map(lambda f_wcl: (f_wcl[0], [(w,c/f_wcl[1][1]) for (w,c) in f_wcl[1][0]])) #normalise the count for each word\n",
    "    return f_wcLn_RDD\n",
    "\n",
    "f_wcLn_RDD = make_f_tfLn_RDD( prefix + 'spam/bare/part1') # for testing\n",
    "print(f_wcLn_RDD.take(1)) # for testing\n",
    "wcLn = f_wcLn_RDD.take(1)[0][1] # get the first normalised word count list\n",
    "print(sum([cn for (w,cn) in wcLn])) # the sum of normalised counts should be close to 1 \n",
    "\n",
    "\n",
    "# Result returns [(filename), [(word, normalised count)....]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task d) Creating hashed feature vectors \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09090909090909091, 0.09090909090909091, 0, 0.09090909090909091, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.045454545454545456, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.045454545454545456, 0, 0, 0, 0, 0, 0, 0, 0, 0.045454545454545456, 0, 0, 0.045454545454545456, 0, 0.045454545454545456, 0, 0, 0.045454545454545456, 0, 0, 0, 0, 0, 0.045454545454545456, 0.045454545454545456, 0, 0, 0, 0.045454545454545456, 0, 0, 0, 0, 0, 0, 0, 0.045454545454545456, 0, 0, 0, 0, 0, 0, 0, 0.045454545454545456, 0, 0, 0, 0.045454545454545456, 0, 0, 0, 0, 0.09090909090909091, 0.045454545454545456, 0, 0, 0.045454545454545456]\n",
      "0.9999999999999997\n"
     ]
    }
   ],
   "source": [
    "def hashing_vectorizer(word_count_list, N): #hash function that creates hash vector for  each tuple\n",
    "    v = [0] * N \n",
    "    for word_count in word_count_list: \n",
    "        word, count = word_count # reorganise tupe by splitting  word and the count\n",
    "        h = hash(word)  #create hash vector for each word in RDD\n",
    "        v[h % N] = v[h % N] + count\n",
    "    return v\n",
    "\n",
    "def make_f_wVn_RDD(f_wcLn_RDD, argN):\n",
    "    file_hash=f_wcLn_RDD.map(lambda f_wc: (f_wc[0], hashing_vectorizer(f_wc[1], argN))) # call hash function to create hash vectors for each word\n",
    "    return file_hash\n",
    "    \n",
    "N=100\n",
    "f_wVn_RDD = make_f_wVn_RDD(make_f_tfLn_RDD(dirPath),N) # for testing\n",
    "print(f_wVn_RDD.take(1)[0][1]) # for testing\n",
    "print( sum(f_wVn_RDD.take(1)[0][1])) # for testing\n",
    "\n",
    "# Result is hashvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task e) Create Labeled Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, [0.0909090909091,0.0909090909091,0.0,0.0909090909091,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0454545454545,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0454545454545,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0454545454545,0.0,0.0,0.0454545454545,0.0,0.0454545454545,0.0,0.0,0.0454545454545,0.0,0.0,0.0,0.0,0.0,0.0454545454545,0.0454545454545,0.0,0.0,0.0,0.0454545454545,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0454545454545,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0454545454545,0.0,0.0,0.0,0.0454545454545,0.0,0.0,0.0,0.0,0.0909090909091,0.0454545454545,0.0,0.0,0.0454545454545]), LabeledPoint(0.0, [0.00691562932227,0.0138312586445,0.00968188105118,0.0235131396957,0.0,0.0124481327801,0.0179806362379,0.0138312586445,0.00276625172891,0.0193637621024,0.030428769018,0.00138312586445,0.00553250345781,0.00138312586445,0.00414937759336,0.00691562932227,0.00691562932227,0.0553250345781,0.00691562932227,0.0110650069156,0.00829875518672,0.0165975103734,0.00691562932227,0.00414937759336,0.0124481327801,0.00276625172891,0.00553250345781,0.00829875518672,0.0,0.0110650069156,0.0179806362379,0.00829875518672,0.00691562932227,0.00414937759336,0.0,0.00414937759336,0.0110650069156,0.00553250345781,0.00138312586445,0.0,0.00968188105118,0.00691562932227,0.0179806362379,0.00276625172891,0.0124481327801,0.00276625172891,0.00138312586445,0.00691562932227,0.00691562932227,0.0207468879668,0.0,0.00414937759336,0.0331950207469,0.015214384509,0.00414937759336,0.00414937759336,0.00968188105118,0.00276625172891,0.00829875518672,0.00414937759336,0.00276625172891,0.0,0.00276625172891,0.00414937759336,0.0110650069156,0.0373443983402,0.00829875518672,0.00414937759336,0.00968188105118,0.0124481327801,0.0401106500692,0.00553250345781,0.0,0.0110650069156,0.0110650069156,0.00968188105118,0.00829875518672,0.0276625172891,0.00968188105118,0.00414937759336,0.0331950207469,0.00276625172891,0.00276625172891,0.0290456431535,0.00553250345781,0.00553250345781,0.00553250345781,0.0138312586445,0.00138312586445,0.0235131396957,0.00138312586445,0.00414937759336,0.00276625172891,0.00691562932227,0.00414937759336,0.0193637621024,0.00691562932227,0.015214384509,0.0207468879668,0.015214384509]), LabeledPoint(0.0, [0.0967741935484,0.0322580645161,0.0,0.0161290322581,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0161290322581,0.0161290322581,0.0,0.0322580645161,0.0483870967742,0.0,0.0322580645161,0.0,0.0161290322581,0.0,0.0,0.0,0.0,0.0,0.0322580645161,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0322580645161,0.0,0.0,0.0,0.0322580645161,0.0161290322581,0.0,0.0,0.0483870967742,0.0,0.0,0.0161290322581,0.0,0.0161290322581,0.0,0.0161290322581,0.0161290322581,0.0,0.0,0.0,0.0161290322581,0.0,0.0,0.0483870967742,0.0,0.0,0.0,0.0161290322581,0.0161290322581,0.0322580645161,0.0,0.0,0.0,0.0,0.0161290322581,0.0,0.0,0.0322580645161,0.0322580645161,0.0,0.0161290322581,0.0161290322581,0.0161290322581,0.0,0.0483870967742,0.0,0.0,0.0,0.0,0.0161290322581,0.0161290322581,0.0161290322581,0.0,0.0,0.0,0.0161290322581,0.0,0.0,0.0,0.0322580645161,0.0,0.0161290322581,0.0161290322581,0.0322580645161])]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def make_lp_RDD(f_tfLn_RDD,argN):\n",
    "    file_hash = make_f_wVn_RDD(f_tfLn_RDD, argN)  # make a vector\n",
    "    lp_RDD = file_hash.map(lambda x: LabeledPoint(1 if re.search('spmsg',x[0]) else 0, x[1])) #detect spam by filename  & transform into LabeledPoint objects\n",
    "    return lp_RDD\n",
    "\n",
    "lp_RDD = make_lp_RDD(make_f_tfLn_RDD(prefix + 'spam/bare/part1'),100)\n",
    "print(lp_RDD.take(3)) \n",
    "\n",
    "#result in the format: LabelledPoint(0 if not spam or 1 if spam[Hashed vectors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task f) Train a classifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data items: 289, correct: 289\n",
      "training accuracy 100.0%\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, NaiveBayes\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "path = prefix + 'spam/stop/part1'\n",
    "\n",
    "N=100\n",
    "def trainModel(f_wcL_RDD,N):\n",
    "    trainData=make_lp_RDD(f_wcL_RDD, N) #call Labelled point function on trianing data\n",
    "    model=LogisticRegressionWithLBFGS.train(trainData) #train Machine Learning algorithm\n",
    "    predicted=trainData.map (lambda x: (x.label, model.predict(x.features))) #classify\n",
    "    count=predicted.count()  #count inputs that are to be classified\n",
    "    correct = predicted.filter(lambda lp: lp[0]==lp[1]).count() #count correctly classified items\n",
    "    accuracy=correct/float(count) #calculate classification accuracy\n",
    "    print('training data items: {}, correct: {}'.format(trainData.count(), correct)) # output raw numbers\n",
    "    print('training accuracy {:.1%}'.format(accuracy)) # and accuracy\n",
    "    return model\n",
    "\n",
    "f_wcLn_RDD = make_f_tfLn_RDD(path) # for testing\n",
    "model = trainModel(f_wcLn_RDD,N) # for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task g) Test the classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data items: 291, correct:219\n",
      "testing accuracy 75.3%\n"
     ]
    }
   ],
   "source": [
    "def testModel(model,f_wcL_RDD,N):\n",
    "    testData=make_lp_RDD(f_wcL_RDD, N) #call labelled point function on test data\n",
    "    predicted=testData.map(lambda x: (x.label, model.predict(x.features))) # classify test data\n",
    "    count=predicted.count() #count total inputs to be classified\n",
    "    correct=predicted.filter(lambda lp: lp[0]==lp[1]).count() # count number correctly classified\n",
    "    accuracy = correct / float(count)   # calculate proportion correctly classified (accuracy)\n",
    "    print('test data items: {}, correct:{}'.format(testData.count(),correct))\n",
    "    print('testing accuracy {:.1%}'.format(accuracy))\n",
    "\n",
    "testModel(model,make_f_tfLn_RDD('hdfs://saltdean/data/spam/stop/part10'),N) # for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Task h) Run experiments \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT 1: Testing different training set sizes\n",
      "Path = hdfs://saltdean/data/spam/bare/part[1-{}], N = 100\n",
      "hdfs://saltdean/data/spam/bare/part[1-1]\n",
      "training data items: 289, correct: 289\n",
      "training accuracy 100.0%\n",
      "test data items: 291, correct:254\n",
      "testing accuracy 87.3%\n",
      "hdfs://saltdean/data/spam/bare/part[1-2]\n",
      "training data items: 578, correct: 578\n",
      "training accuracy 100.0%\n",
      "test data items: 291, correct:266\n",
      "testing accuracy 91.4%\n",
      "hdfs://saltdean/data/spam/bare/part[1-3]\n",
      "training data items: 867, correct: 867\n",
      "training accuracy 100.0%\n",
      "test data items: 291, correct:265\n",
      "testing accuracy 91.1%\n",
      "hdfs://saltdean/data/spam/bare/part[1-4]\n",
      "training data items: 1156, correct: 1156\n",
      "training accuracy 100.0%\n",
      "test data items: 291, correct:265\n",
      "testing accuracy 91.1%\n",
      "hdfs://saltdean/data/spam/bare/part[1-5]\n",
      "training data items: 1446, correct: 1446\n",
      "training accuracy 100.0%\n",
      "test data items: 291, correct:264\n",
      "testing accuracy 90.7%\n",
      "hdfs://saltdean/data/spam/bare/part[1-6]\n",
      "training data items: 1735, correct: 1708\n",
      "training accuracy 98.4%\n",
      "test data items: 291, correct:274\n",
      "testing accuracy 94.2%\n",
      "hdfs://saltdean/data/spam/bare/part[1-7]\n",
      "training data items: 2024, correct: 1990\n",
      "training accuracy 98.3%\n",
      "test data items: 291, correct:280\n",
      "testing accuracy 96.2%\n",
      "hdfs://saltdean/data/spam/bare/part[1-8]\n",
      "training data items: 2313, correct: 2258\n",
      "training accuracy 97.6%\n",
      "test data items: 291, correct:275\n",
      "testing accuracy 94.5%\n",
      "hdfs://saltdean/data/spam/bare/part[1-9]\n",
      "training data items: 2602, correct: 2540\n",
      "training accuracy 97.6%\n",
      "test data items: 291, correct:276\n",
      "testing accuracy 94.8%\n",
      "\n",
      "EXPERIMENT 2: Testing different vector sizes\n",
      "=== N =  3\n",
      "training data items: 2602, correct: 2165\n",
      "training accuracy 83.2%\n",
      "test data items: 291, correct:233\n",
      "testing accuracy 80.1%\n",
      "=== N =  10\n",
      "training data items: 2602, correct: 2194\n",
      "training accuracy 84.3%\n",
      "test data items: 291, correct:248\n",
      "testing accuracy 85.2%\n",
      "=== N =  30\n",
      "training data items: 2602, correct: 2354\n",
      "training accuracy 90.5%\n",
      "test data items: 291, correct:255\n",
      "testing accuracy 87.6%\n",
      "=== N =  100\n",
      "training data items: 2602, correct: 2540\n",
      "training accuracy 97.6%\n",
      "test data items: 291, correct:276\n",
      "testing accuracy 94.8%\n",
      "=== N =  300\n",
      "training data items: 2602, correct: 2602\n",
      "training accuracy 100.0%\n",
      "test data items: 291, correct:284\n",
      "testing accuracy 97.6%\n",
      "\n",
      "EXPERIMENT 3: Testing differently preprocessed data sets\n",
      "training on parts 1-9, N = 100\n",
      "===  Stopwords removed\n",
      "hdfs://saltdean.nsqdc.city.ac.uk/data/spam/stop/part[1-9]\n",
      "training data items: 2602, correct: 2425\n",
      "training accuracy 93.2%\n",
      "test data items: 291, correct:265\n",
      "testing accuracy 91.1%\n",
      "===  Lemmatised\n",
      "hdfs://saltdean.nsqdc.city.ac.uk/data/spam/lemm/part[1-9]\n",
      "training data items: 2602, correct: 2541\n",
      "training accuracy 97.7%\n",
      "test data items: 291, correct:271\n",
      "testing accuracy 93.1%\n",
      "===  No preprocessing\n",
      "hdfs://saltdean.nsqdc.city.ac.uk/data/spam/bare/part[1-9]\n",
      "training data items: 2602, correct: 2540\n",
      "training accuracy 97.6%\n",
      "test data items: 291, correct:276\n",
      "testing accuracy 94.8%\n",
      "===  Lemmatised and stopwords removed\n",
      "hdfs://saltdean.nsqdc.city.ac.uk/data/spam/lemm_stop/part[1-9]\n",
      "training data items: 2602, correct: 2420\n",
      "training accuracy 93.0%\n",
      "test data items: 291, correct:255\n",
      "testing accuracy 87.6%\n",
      "\n",
      "====== Done ======\n"
     ]
    }
   ],
   "source": [
    "# this function combines tasks f) and g)\n",
    "def trainTestModel(trainPaths,testPath,N):\n",
    "    model=trainModel(make_f_tfLn_RDD(trainPaths),N) #combine training and testing \n",
    "    testModel(model,make_f_tfLn_RDD(testPath),N)\n",
    "    return model                 \n",
    "                \n",
    "# prepare the part directories and the path\n",
    "dirPattern = 'hdfs://saltdean/data/spam/bare/part[1-{}]' # the {} can be filled by 'dirPattern.format(i)' \n",
    "# create the path for the test set\n",
    "testPath = 'hdfs://saltdean/data/spam/bare/part10'\n",
    "\n",
    "print('EXPERIMENT 1: Testing different training set sizes')\n",
    "print('Path = {}, N = {}'.format(dirPattern,N)) # using format to make sure we record the parameters of the experiment\n",
    "#<<< make the test set, it will be constant for this experiment\n",
    "for i in range(1,10): # loop over i the number of parts for training (1-9)\n",
    "    trainPaths = dirPattern.format(i) # in the loop you can create a path like this\n",
    "    print(trainPaths) #just for testing, remove later\n",
    "    trainTestModel(trainPaths,testPath,N) # create the trainRDD (using your make_f_tfLn_RDD method)\n",
    "    \n",
    "\n",
    "print('\\nEXPERIMENT 2: Testing different vector sizes')\n",
    "array= [3,10,30,100,300] #<<< loop over different values for N. 3,10,30,100,300, ... is a good pattern\n",
    "for i in range (0,5):\n",
    "    print('=== N = ', array[i])\n",
    "    trainTestModel(trainPaths,testPath,array[i])\n",
    "\n",
    "N = 100 # change to what you feel is a good compromise between computation and accuracy\n",
    "# the dictionary below helps associate description and paths.\n",
    "setDict = {'No preprocessing': prefix + 'spam/bare/',\n",
    "           'Stopwords removed': prefix + 'spam/stop/',\n",
    "           'Lemmatised': prefix + 'spam/lemm/',\n",
    "           'Lemmatised and stopwords removed': prefix + 'spam/lemm_stop/'}\n",
    "\n",
    "print('\\nEXPERIMENT 3: Testing differently preprocessed data sets')\n",
    "print('training on parts 1-9, N = {}'.format(N))\n",
    "for sp in setDict:\n",
    "    print('=== ',sp)\n",
    "    trainPaths = setDict[sp] + 'part[1-9]' #make training data 1-9\n",
    "    print(trainPaths)\n",
    "    trainTestModel(trainPaths, testPath, N) # test data\n",
    "    \n",
    "print('\\n====== Done ======')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
